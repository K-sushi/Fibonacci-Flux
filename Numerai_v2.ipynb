{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNXgLHB0FVmPomerPyl+FfI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/K-sushi/Fibonacci-Flux/blob/main/Numerai_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q numerapi pandas pyarrow matplotlib lightgbm scikit-learn cloudpickle==2.2.1 seaborn scipy==1.10.1\n",
        "\n",
        "# Inline plots\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0dUPbzV_hVj",
        "outputId": "f1bc524b-4ec3-4463-f62e-779019ec2b3e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfj3Vu3afCVm",
        "outputId": "5c4616ef-43d0-496e-a848-d94b088332f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scipy 1.10.1 requires numpy<1.27.0,>=1.19.5, but you have numpy 2.2.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pandas scipy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxhZGOUZfF3h",
        "outputId": "e9d149e9-657b-4658-f587-0fd2cc112f73"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "Successfully installed scipy-1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numerapi pandas pyarrow matplotlib lightgbm scikit-learn seaborn scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6qpT9XsfIaE",
        "outputId": "e017176b-195e-42c4-f721-32d486c86310"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numerapi in /usr/local/lib/python3.11/dist-packages (2.20.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (19.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from numerapi) (2.32.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from numerapi) (2025.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from numerapi) (2.9.0.post0)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.11/dist-packages (from numerapi) (4.67.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from numerapi) (8.1.8)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.2.5)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->numerapi) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->numerapi) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->numerapi) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->numerapi) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->numerapi) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall numpy pandas scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8V00LfOtfeJ1",
        "outputId": "51c80813-1b95-4044-e40c-87f3e3820c91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, tzdata, six, numpy, scipy, python-dateutil, pandas\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.3\n",
            "    Uninstalling pandas-2.2.3:\n",
            "      Successfully uninstalled pandas-2.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 scipy-1.15.2 six-1.17.0 tzdata-2025.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "six"
                ]
              },
              "id": "35fa57cfe2a24bcfb992ea1ce9008e5c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJjRwyhgfjsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "xDDQ7enF_dTf",
        "outputId": "baa6a804-5513-4c9d-b6a4-d2ad7bc99d7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   era  target_agnes_20  target_agnes_60  target_alpha_20  \\\n",
              "id                                                                          \n",
              "n0007b5abb0c3a25  0001             0.25             0.00             0.25   \n",
              "n003bba8a98662e4  0001             0.25             0.25             0.25   \n",
              "n003bee128c2fcfc  0001             1.00             1.00             1.00   \n",
              "n0048ac83aff7194  0001             0.25             0.25             0.25   \n",
              "n0055a2401ba6480  0001             0.25             0.50             0.25   \n",
              "...                ...              ...              ...              ...   \n",
              "nffc2d5e4b79a7ae  0573             0.00             0.25             0.00   \n",
              "nffc9844c1c7a6a9  0573             0.50             0.50             0.25   \n",
              "nffd79773f4109bb  0573             0.50             0.50             0.75   \n",
              "nfff6ab9d6dc0b32  0573             0.50             0.50             0.25   \n",
              "nfff87b21e4db902  0573             0.75             0.75             0.50   \n",
              "\n",
              "                  target_alpha_60  target_bravo_20  target_bravo_60  \\\n",
              "id                                                                    \n",
              "n0007b5abb0c3a25             0.25             0.00             0.00   \n",
              "n003bba8a98662e4             0.00             0.25             0.00   \n",
              "n003bee128c2fcfc             1.00             0.75             1.00   \n",
              "n0048ac83aff7194             0.25             0.50             0.25   \n",
              "n0055a2401ba6480             0.50             0.25             0.50   \n",
              "...                           ...              ...              ...   \n",
              "nffc2d5e4b79a7ae             0.25             0.00             0.25   \n",
              "nffc9844c1c7a6a9             0.50             0.50             0.50   \n",
              "nffd79773f4109bb             0.50             0.75             0.50   \n",
              "nfff6ab9d6dc0b32             0.50             0.50             0.50   \n",
              "nfff87b21e4db902             0.75             0.50             0.75   \n",
              "\n",
              "                  target_caroline_20  target_caroline_60  target_charlie_20  \\\n",
              "id                                                                            \n",
              "n0007b5abb0c3a25                0.25                0.00               0.25   \n",
              "n003bba8a98662e4                0.25                0.25               0.25   \n",
              "n003bee128c2fcfc                0.75                0.75               0.75   \n",
              "n0048ac83aff7194                0.50                0.25               0.50   \n",
              "n0055a2401ba6480                0.25                0.50               0.25   \n",
              "...                              ...                 ...                ...   \n",
              "nffc2d5e4b79a7ae                0.25                0.50               0.00   \n",
              "nffc9844c1c7a6a9                0.50                0.50               0.50   \n",
              "nffd79773f4109bb                0.50                0.50               0.75   \n",
              "nfff6ab9d6dc0b32                0.25                0.50               0.25   \n",
              "nfff87b21e4db902                0.50                0.50               0.50   \n",
              "\n",
              "                  ...  target_teager2b_60  target_tyler_20  target_tyler_60  \\\n",
              "id                ...                                                         \n",
              "n0007b5abb0c3a25  ...                0.50             0.25             0.25   \n",
              "n003bba8a98662e4  ...                0.50             0.25             0.25   \n",
              "n003bee128c2fcfc  ...                1.00             1.00             0.75   \n",
              "n0048ac83aff7194  ...                0.25             0.25             0.25   \n",
              "n0055a2401ba6480  ...                0.50             0.25             0.50   \n",
              "...               ...                 ...              ...              ...   \n",
              "nffc2d5e4b79a7ae  ...                0.50             0.25             0.50   \n",
              "nffc9844c1c7a6a9  ...                0.75             0.50             0.50   \n",
              "nffd79773f4109bb  ...                0.75             0.50             0.50   \n",
              "nfff6ab9d6dc0b32  ...                0.50             0.50             0.25   \n",
              "nfff87b21e4db902  ...                0.75             0.75             0.75   \n",
              "\n",
              "                  target_victor_20  target_victor_60  target_waldo_20  \\\n",
              "id                                                                      \n",
              "n0007b5abb0c3a25              0.25              0.25             0.25   \n",
              "n003bba8a98662e4              0.25              0.00             0.25   \n",
              "n003bee128c2fcfc              0.75              0.75             0.75   \n",
              "n0048ac83aff7194              0.50              0.25             0.25   \n",
              "n0055a2401ba6480              0.25              0.50             0.25   \n",
              "...                            ...               ...              ...   \n",
              "nffc2d5e4b79a7ae              0.25              0.50             0.00   \n",
              "nffc9844c1c7a6a9              0.50              0.50             0.50   \n",
              "nffd79773f4109bb              0.50              0.50             0.50   \n",
              "nfff6ab9d6dc0b32              0.25              0.50             0.50   \n",
              "nfff87b21e4db902              0.50              0.75             0.50   \n",
              "\n",
              "                  target_waldo_60  target_xerxes_20  target_xerxes_60  target  \n",
              "id                                                                             \n",
              "n0007b5abb0c3a25             0.00              0.25              0.00    0.25  \n",
              "n003bba8a98662e4             0.25              0.25              0.25    0.25  \n",
              "n003bee128c2fcfc             1.00              0.75              0.75    0.75  \n",
              "n0048ac83aff7194             0.25              0.25              0.25    0.25  \n",
              "n0055a2401ba6480             0.50              0.25              0.50    0.25  \n",
              "...                           ...               ...               ...     ...  \n",
              "nffc2d5e4b79a7ae             0.50              0.00              0.25    0.00  \n",
              "nffc9844c1c7a6a9             0.50              0.50              0.50    0.25  \n",
              "nffd79773f4109bb             0.75              0.50              0.50    0.50  \n",
              "nfff6ab9d6dc0b32             0.50              0.25              0.50    0.25  \n",
              "nfff87b21e4db902             0.50              0.50              0.50    0.50  \n",
              "\n",
              "[688184 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d1d8fd1-609b-48ad-a5d2-a663e59ad185\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>era</th>\n",
              "      <th>target_agnes_20</th>\n",
              "      <th>target_agnes_60</th>\n",
              "      <th>target_alpha_20</th>\n",
              "      <th>target_alpha_60</th>\n",
              "      <th>target_bravo_20</th>\n",
              "      <th>target_bravo_60</th>\n",
              "      <th>target_caroline_20</th>\n",
              "      <th>target_caroline_60</th>\n",
              "      <th>target_charlie_20</th>\n",
              "      <th>...</th>\n",
              "      <th>target_teager2b_60</th>\n",
              "      <th>target_tyler_20</th>\n",
              "      <th>target_tyler_60</th>\n",
              "      <th>target_victor_20</th>\n",
              "      <th>target_victor_60</th>\n",
              "      <th>target_waldo_20</th>\n",
              "      <th>target_waldo_60</th>\n",
              "      <th>target_xerxes_20</th>\n",
              "      <th>target_xerxes_60</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>n0007b5abb0c3a25</th>\n",
              "      <td>0001</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n003bba8a98662e4</th>\n",
              "      <td>0001</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n003bee128c2fcfc</th>\n",
              "      <td>0001</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n0048ac83aff7194</th>\n",
              "      <td>0001</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n0055a2401ba6480</th>\n",
              "      <td>0001</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nffc2d5e4b79a7ae</th>\n",
              "      <td>0573</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nffc9844c1c7a6a9</th>\n",
              "      <td>0573</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nffd79773f4109bb</th>\n",
              "      <td>0573</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nfff6ab9d6dc0b32</th>\n",
              "      <td>0573</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nfff87b21e4db902</th>\n",
              "      <td>0573</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>688184 rows × 38 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d1d8fd1-609b-48ad-a5d2-a663e59ad185')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3d1d8fd1-609b-48ad-a5d2-a663e59ad185 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3d1d8fd1-609b-48ad-a5d2-a663e59ad185');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5f62e6c7-0e8d-452c-9c8e-bfffc2bef78f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5f62e6c7-0e8d-452c-9c8e-bfffc2bef78f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5f62e6c7-0e8d-452c-9c8e-bfffc2bef78f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from numerapi import NumerAPI\n",
        "\n",
        "# Set the data version to one of the most recent versions\n",
        "DATA_VERSION = \"v5.0\"\n",
        "MAIN_TARGET = \"target_cyrusd_20\"\n",
        "TARGET_CANDIDATES = [\n",
        "  MAIN_TARGET,\n",
        "  \"target_victor_20\",\n",
        "  \"target_xerxes_20\",\n",
        "  \"target_teager2b_20\"\n",
        "]\n",
        "FAVORITE_MODEL = \"v5_lgbm_ct_blend\"\n",
        "\n",
        "# Download data\n",
        "napi = NumerAPI()\n",
        "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
        "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
        "napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
        "napi.download_dataset(f\"{DATA_VERSION}/live_example_preds.parquet\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load data\n",
        "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
        "feature_cols = feature_metadata[\"feature_sets\"][\"medium\"]\n",
        "target_cols = feature_metadata[\"targets\"]\n",
        "train = pd.read_parquet(f\"{DATA_VERSION}/train.parquet\")\n",
        "\n",
        "# Downsample to every 4th era to reduce memory usage and speedup model training (suggested for Colab free tier)\n",
        "# Comment out the line below to use all the data (higher memory usage, slower model training, potentially better performance)\n",
        "train = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]\n",
        "\n",
        "# Print target columns\n",
        "train[[\"era\"] + target_cols]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from google.colab import drive\n",
        "import gc\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "\n",
        "# 警告を抑制\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def setup_numerai_environment(mount_drive=True, drive_path='MyDrive/Numerai',\n",
        "                             use_api=True, api_key=None, api_secret=None,\n",
        "                             cache_data=True):\n",
        "    \"\"\"\n",
        "    Numerai環境をGoogle Colab上にセットアップする関数\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    mount_drive : bool\n",
        "        Googleドライブをマウントするかどうか\n",
        "    drive_path : str\n",
        "        Googleドライブ上のデータ保存パス\n",
        "    use_api : bool\n",
        "        Numerai APIを使用するかどうか\n",
        "    api_key : str, optional\n",
        "        Numerai API Key（なくても基本的なダウンロードは可能）\n",
        "    api_secret : str, optional\n",
        "        Numerai API Secret（なくても基本的なダウンロードは可能）\n",
        "    cache_data : bool\n",
        "        データをキャッシュするかどうか\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        設定情報と保存パスを含む辞書\n",
        "    \"\"\"\n",
        "    config = {'data_path': '/content'}\n",
        "\n",
        "    # Googleドライブのマウント\n",
        "    if mount_drive:\n",
        "        print(\"Googleドライブをマウント中...\")\n",
        "        drive.mount('/content/drive')\n",
        "        drive_data_path = f'/content/drive/{drive_path}'\n",
        "        os.makedirs(drive_data_path, exist_ok=True)\n",
        "        config['drive_path'] = drive_data_path\n",
        "        print(f\"ドライブパス: {drive_data_path}\")\n",
        "\n",
        "    # Numerai APIのセットアップ\n",
        "    if use_api:\n",
        "        try:\n",
        "            # numerapiのインストール（未インストールの場合）\n",
        "            try:\n",
        "                import numerapi\n",
        "            except ImportError:\n",
        "                print(\"numerapiをインストール中...\")\n",
        "                !pip install -q numerapi\n",
        "                import numerapi\n",
        "\n",
        "            # API接続のセットアップ\n",
        "            if api_key and api_secret:\n",
        "                napi = numerapi.NumerAPI(api_key, api_secret)\n",
        "                print(\"APIキーを使用してNumerai APIに接続しました\")\n",
        "            else:\n",
        "                napi = numerapi.NumerAPI()\n",
        "                print(\"公開APIとしてNumerai APIに接続しました\")\n",
        "\n",
        "            config['napi'] = napi\n",
        "            current_round = napi.get_current_round()\n",
        "            print(f\"現在のラウンド: {current_round}\")\n",
        "        except Exception as e:\n",
        "            print(f\"APIセットアップエラー: {e}\")\n",
        "            use_api = False\n",
        "\n",
        "    # キャッシュディレクトリの作成\n",
        "    if cache_data:\n",
        "        cache_path = '/content/numerai_cache'\n",
        "        os.makedirs(cache_path, exist_ok=True)\n",
        "        config['cache_path'] = cache_path\n",
        "\n",
        "    return config\n",
        "\n",
        "def download_numerai_data(config, datasets=None, force_download=False, save_to_drive=True):\n",
        "    \"\"\"\n",
        "    Numeraiデータセットをダウンロードする関数\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    config : dict\n",
        "        セットアップ関数から返される設定情報\n",
        "    datasets : list, optional\n",
        "        ダウンロードするデータセットのリスト\n",
        "    force_download : bool\n",
        "        既存のファイルを上書きするかどうか\n",
        "    save_to_drive : bool\n",
        "        Googleドライブに保存するかどうか\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        データセットパスの辞書\n",
        "    \"\"\"\n",
        "    if datasets is None:\n",
        "        datasets = [\n",
        "            \"numerai_training_data.parquet\",\n",
        "            \"numerai_tournament_data.parquet\",\n",
        "            \"numerai_validation_data.parquet\",\n",
        "            \"example_predictions.parquet\",\n",
        "            \"features.json\"\n",
        "        ]\n",
        "\n",
        "    # ファイル名の短縮マッピング\n",
        "    filename_map = {\n",
        "        \"numerai_training_data.parquet\": \"train.parquet\",\n",
        "        \"numerai_tournament_data.parquet\": \"live.parquet\",\n",
        "        \"numerai_validation_data.parquet\": \"validation.parquet\",\n",
        "        \"example_predictions.parquet\": \"example_preds.parquet\",\n",
        "        \"features.json\": \"features.json\"\n",
        "    }\n",
        "\n",
        "    dataset_paths = {}\n",
        "    napi = config.get('napi')\n",
        "\n",
        "    if napi is None:\n",
        "        print(\"有効なNumerai APIインスタンスがありません\")\n",
        "        return dataset_paths\n",
        "\n",
        "    for dataset in datasets:\n",
        "        short_name = filename_map.get(dataset, dataset)\n",
        "        local_path = f\"/content/{short_name}\"\n",
        "\n",
        "        # 最初にローカルに保存\n",
        "        if not os.path.exists(local_path) or force_download:\n",
        "            print(f\"{dataset}をダウンロード中...\")\n",
        "            try:\n",
        "                napi.download_dataset(dataset, local_path)\n",
        "                print(f\"  保存先: {local_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ダウンロードエラー: {e}\")\n",
        "                continue\n",
        "        else:\n",
        "            print(f\"{short_name}は既にダウンロードされています\")\n",
        "\n",
        "        dataset_paths[short_name] = local_path\n",
        "\n",
        "        # Googleドライブにコピー（オプション）\n",
        "        if save_to_drive and 'drive_path' in config:\n",
        "            drive_path = f\"{config['drive_path']}/{short_name}\"\n",
        "            try:\n",
        "                if not os.path.exists(drive_path) or force_download:\n",
        "                    print(f\"  Googleドライブにコピー中: {drive_path}\")\n",
        "                    !cp \"{local_path}\" \"{drive_path}\"\n",
        "            except Exception as e:\n",
        "                print(f\"  ドライブへのコピーエラー: {e}\")\n",
        "\n",
        "    # features.jsonの読み込み\n",
        "    if 'features.json' in dataset_paths:\n",
        "        try:\n",
        "            with open(dataset_paths['features.json'], 'r') as f:\n",
        "                config['features_info'] = json.load(f)\n",
        "            print(\"features.jsonを読み込みました\")\n",
        "        except Exception as e:\n",
        "            print(f\"features.json読み込みエラー: {e}\")\n",
        "\n",
        "    return dataset_paths\n",
        "\n",
        "def load_numerai_data(dataset_paths, config=None):\n",
        "    \"\"\"\n",
        "    ダウンロードしたNumeraiデータを読み込む関数\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dataset_paths : dict\n",
        "        ダウンロード関数から返されるデータセットパスの辞書\n",
        "    config : dict, optional\n",
        "        設定情報\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        データフレームの辞書\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "\n",
        "    # 主要データセットの読み込み\n",
        "    datasets_to_load = {\n",
        "        'train.parquet': 'train_df',\n",
        "        'live.parquet': 'tournament_df',\n",
        "        'validation.parquet': 'validation_df',\n",
        "        'example_preds.parquet': 'example_preds'\n",
        "    }\n",
        "\n",
        "    for file_key, df_key in datasets_to_load.items():\n",
        "        if file_key in dataset_paths:\n",
        "            try:\n",
        "                print(f\"{file_key}を読み込み中...\")\n",
        "                data[df_key] = pd.read_parquet(dataset_paths[file_key])\n",
        "                print(f\"  形状: {data[df_key].shape}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  読み込みエラー: {e}\")\n",
        "\n",
        "    # 特徴量の検出\n",
        "    if 'train_df' in data:\n",
        "        data['feature_cols'] = [col for col in data['train_df'].columns\n",
        "                               if col.startswith('feature')]\n",
        "        print(f\"検出された特徴量数: {len(data['feature_cols'])}\")\n",
        "\n",
        "    # 設定情報に特徴量グループが含まれている場合はそれを追加\n",
        "    if config and 'features_info' in config:\n",
        "        data['features_info'] = config['features_info']\n",
        "        if 'feature_sets' in config['features_info']:\n",
        "            print(\"特徴量セット情報を読み込みました\")\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "OCobEWXKKLa6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 通常のダウンサンプリング（現在の方法）\n",
        "train = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]\n",
        "\n",
        "# より洗練されたサンプリング戦略\n",
        "def smart_sampling(df, era_col=\"era\", target_col=MAIN_TARGET, sample_ratio=0.25):\n",
        "    \"\"\"より賢いサンプリング方法：時間的連続性と分布の保持を重視\"\"\"\n",
        "    unique_eras = df[era_col].unique()\n",
        "\n",
        "    # 最新エラを優先サンプリング（より最近のデータは価値が高い）\n",
        "    recent_eras = unique_eras[-int(len(unique_eras) * 0.3):]  # 最新30%\n",
        "    older_eras = unique_eras[:-int(len(unique_eras) * 0.3)]   # 残り70%\n",
        "\n",
        "    # 最新エラは高い比率で、古いエラは低い比率でサンプリング\n",
        "    sampled_recent = recent_eras[::max(1, int(1/sample_ratio * 0.5))]  # 最新エラは2倍の密度\n",
        "    sampled_older = older_eras[::max(1, int(1/sample_ratio * 1.5))]    # 古いエラは2/3の密度\n",
        "\n",
        "    sampled_eras = np.concatenate([sampled_older, sampled_recent])\n",
        "    return df[df[era_col].isin(sampled_eras)]\n",
        "\n",
        "train = smart_sampling(train, sample_ratio=0.25)  # 全体の25%を使用"
      ],
      "metadata": {
        "id": "ux3HVyVeAe7t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ultra_efficient_feature_engineering(df, feature_cols, era_col=\"era\", features_info=None):\n",
        "    \"\"\"計算効率を極限まで高めた特徴エンジニアリング\"\"\"\n",
        "    print(\"特徴エンジニアリングを開始...\")\n",
        "    result_df = df.copy()\n",
        "\n",
        "    # 実際にデータフレームに存在する特徴量のみをフィルタリング\n",
        "    valid_feature_cols = [col for col in feature_cols if col in df.columns]\n",
        "    print(f\"有効な特徴量: {len(valid_feature_cols)}/{len(feature_cols)}\")\n",
        "\n",
        "    if len(valid_feature_cols) == 0:\n",
        "        print(\"警告: 有効な特徴量が見つかりません。データを確認してください。\")\n",
        "        return result_df\n",
        "\n",
        "    # 1. メモリ最適化: float64からfloat32へ変換\n",
        "    for col in valid_feature_cols:\n",
        "        if result_df[col].dtype == 'float64':\n",
        "            result_df[col] = result_df[col].astype('float32')\n",
        "\n",
        "    # 2. 最も効果的な変換だけを適用（ランク変換と正規化）\n",
        "    for era in df[era_col].unique():\n",
        "        era_data = df[df[era_col] == era]\n",
        "\n",
        "        # 特徴重要度情報があれば、それを使用\n",
        "        if features_info and 'feature_stats' in features_info:\n",
        "            # 存在する特徴だけをフィルタリング\n",
        "            valid_stats = {f: features_info['feature_stats'][f]\n",
        "                          for f in valid_feature_cols\n",
        "                          if f in features_info['feature_stats']}\n",
        "\n",
        "            if valid_stats:\n",
        "                top_features = sorted(\n",
        "                    [(f, valid_stats[f].get('corr', 0)) for f in valid_stats],\n",
        "                    key=lambda x: abs(x[1]),\n",
        "                    reverse=True\n",
        "                )[:min(50, len(valid_stats))]  # 上位50特徴または全特徴\n",
        "                selected_features = [f for f, _ in top_features]\n",
        "            else:\n",
        "                selected_features = valid_feature_cols[:min(50, len(valid_feature_cols))]\n",
        "        else:\n",
        "            selected_features = valid_feature_cols[:min(50, len(valid_feature_cols))]\n",
        "\n",
        "        # 最も効果的なランク変換だけを適用\n",
        "        for col in selected_features[:min(15, len(selected_features))]:  # 最大15特徴\n",
        "            result_df.loc[result_df[era_col] == era, f'{col}_rank'] = \\\n",
        "                era_data[col].rank(pct=True).astype('float32')\n",
        "\n",
        "        # 最も効果的なエラ内正規化だけを適用\n",
        "        for col in selected_features[:min(20, len(selected_features))]:  # 最大20特徴\n",
        "            mean = era_data[col].mean()\n",
        "            std = era_data[col].std()\n",
        "            if std > 0:\n",
        "                result_df.loc[result_df[era_col] == era, f'{col}_norm'] = \\\n",
        "                    ((result_df.loc[result_df[era_col] == era, col] - mean) / std).astype('float32')\n",
        "\n",
        "    # 3. 最小限のクロス特徴（相互作用）\n",
        "    top_pairs = [\n",
        "        ('intelligence', 'strength'),\n",
        "        ('wisdom', 'charisma')\n",
        "    ]\n",
        "\n",
        "    # グループ情報がある場合のみ実行\n",
        "    if features_info and 'feature_sets' in features_info:\n",
        "        for group1, group2 in top_pairs:\n",
        "            if group1 in features_info['feature_sets'] and group2 in features_info['feature_sets']:\n",
        "                # 実際にデータフレームに存在する特徴のみをフィルタリング\n",
        "                g1_features = [f for f in features_info['feature_sets'].get(group1, [])[:3]\n",
        "                              if f in df.columns]  # 最大3特徴\n",
        "                g2_features = [f for f in features_info['feature_sets'].get(group2, [])[:3]\n",
        "                              if f in df.columns]  # 最大3特徴\n",
        "\n",
        "                for f1 in g1_features:\n",
        "                    for f2 in g2_features:\n",
        "                        result_df[f'{f1}_mul_{f2}'] = (result_df[f1] * result_df[f2]).astype('float32')\n",
        "\n",
        "    # 4. シンプルなグループ統計（最も重要なグループのみ）\n",
        "    if features_info and 'feature_sets' in features_info:\n",
        "        for group in ['intelligence', 'strength']:  # 最重要グループのみ\n",
        "            if group in features_info['feature_sets']:\n",
        "                # 実際にデータフレームに存在する特徴のみをフィルタリング\n",
        "                group_features = [f for f in features_info['feature_sets'][group][:20]\n",
        "                                 if f in df.columns]  # 最大20特徴\n",
        "                if len(group_features) >= 5:\n",
        "                    result_df[f'mean_{group}'] = result_df[group_features].mean(axis=1).astype('float32')\n",
        "\n",
        "    # キャスト後のメモリ使用量\n",
        "    num_bytes = result_df.memory_usage(deep=True).sum()\n",
        "    print(f\"メモリ使用量: {num_bytes / 1e6:.2f} MB\")\n",
        "\n",
        "    print(\"特徴エンジニアリング完了\")\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "uEx8u57nKS-N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ultra_fast_feature_selection(df, target_col, feature_cols, era_col=\"era\"):\n",
        "    \"\"\"超高速な特徴選択（計算コスト最小化）\"\"\"\n",
        "    from sklearn.feature_selection import SelectFromModel\n",
        "    from sklearn.linear_model import LassoCV  # LightGBMより高速\n",
        "\n",
        "    # 特徴のフィルタリング（より計算効率の高い方法）\n",
        "    all_features = [col for col in df.columns\n",
        "                   if col.startswith('feature') or\n",
        "                   '_rank' in col or '_norm' in col or\n",
        "                   '_mul_' in col or 'pca_' in col or\n",
        "                   col.startswith('mean_')]\n",
        "\n",
        "    # サンプリングによるメモリ効率化\n",
        "    sample_size = min(50000, len(df))\n",
        "    sample_idx = np.random.choice(len(df), sample_size, replace=False)\n",
        "    sample_df = df.iloc[sample_idx]\n",
        "\n",
        "    # 高速な特徴選択モデル\n",
        "    selector = LassoCV(cv=3, max_iter=100, tol=0.01)\n",
        "    selector.fit(sample_df[all_features], sample_df[target_col])\n",
        "\n",
        "    # 重要な特徴の選択\n",
        "    importance = np.abs(selector.coef_)\n",
        "    feature_importance = pd.Series(importance, index=all_features)\n",
        "    feature_importance = feature_importance.sort_values(ascending=False)\n",
        "\n",
        "    # 上位特徴のみを選択\n",
        "    top_features = feature_importance.index[:200].tolist()  # 200特徴に制限\n",
        "\n",
        "    return top_features\n",
        "\n",
        "def train_minimal_models(train_df, features, target_col, era_col=\"era\"):\n",
        "    \"\"\"リソース制約下での最小限のモデルトレーニング\"\"\"\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from lightgbm import LGBMRegressor\n",
        "\n",
        "    # サンプリングによるメモリ効率化\n",
        "    sample_size = min(50000, len(train_df))\n",
        "    sample_idx = np.random.choice(len(train_df), sample_size, replace=False)\n",
        "    sample_df = train_df.iloc[sample_idx]\n",
        "\n",
        "    # 特徴とターゲットの取得\n",
        "    X = sample_df[features]\n",
        "    y = sample_df[target_col]\n",
        "\n",
        "    # 1. Ridge回帰（計算効率が非常に高い）\n",
        "    ridge_model = Ridge(alpha=1.0, random_state=42)\n",
        "    ridge_model.fit(X, y)\n",
        "\n",
        "    # 2. 軽量LightGBM（パラメータを最小化）\n",
        "    lgbm_model = LGBMRegressor(\n",
        "        n_estimators=500,  # 1000から半減\n",
        "        learning_rate=0.02,  # 学習率を上げて反復回数削減\n",
        "        num_leaves=15,     # 葉の数を減らす\n",
        "        max_depth=4,       # 深さを制限\n",
        "        colsample_bytree=0.7,\n",
        "        subsample=0.7,\n",
        "        verbose=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    lgbm_model.fit(X, y)\n",
        "\n",
        "    # モデルをまとめる\n",
        "    models = {\n",
        "        'ridge': ridge_model,\n",
        "        'lgbm': lgbm_model,\n",
        "        'weights': {'ridge': 0.3, 'lgbm': 0.7}  # 経験的に良い比率\n",
        "    }\n",
        "\n",
        "    return models"
      ],
      "metadata": {
        "id": "WexjYWc9KVVy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def streamlined_prediction_optimization(predictions_df, feature_df, era_col=\"era\",\n",
        "                                       example_preds=None, feature_groups=None):\n",
        "    \"\"\"リソース効率を最大化した予測最適化\"\"\"\n",
        "    result_df = predictions_df.copy()\n",
        "\n",
        "    # 1. 効率的なエラ内正規化\n",
        "    for era in result_df[era_col].unique():\n",
        "        era_mask = result_df[era_col] == era\n",
        "        if era_mask.sum() >= 2:\n",
        "            era_preds = result_df.loc[era_mask, 'prediction']\n",
        "            mean = np.mean(era_preds)\n",
        "            std = np.std(era_preds)\n",
        "            if std > 0:\n",
        "                result_df.loc[era_mask, 'prediction'] = (era_preds - mean) / std\n",
        "\n",
        "    # 2. 最小限の特徴中立化（最も重要な特徴グループのみ）\n",
        "    if feature_df is not None and feature_groups:\n",
        "        # 最も重要なグループだけを中立化\n",
        "        neutralize_groups = {\n",
        "            'intelligence': 0.5,  # 最重要グループ\n",
        "            'strength': 0.5       # 最重要グループ\n",
        "        }\n",
        "\n",
        "        for group_name, proportion in neutralize_groups.items():\n",
        "            if group_name in feature_groups and len(feature_groups[group_name]) > 0:\n",
        "                print(f\"  {group_name}グループを中立化中...\")\n",
        "                neutral_cols = feature_groups[group_name][:50]  # 上位50特徴に制限\n",
        "\n",
        "                # 一度にすべてのエラを処理（バッチ処理をスキップしてコード簡略化）\n",
        "                from sklearn.linear_model import Ridge  # LinearRegressionより安定\n",
        "\n",
        "                for era in result_df[era_col].unique():\n",
        "                    era_mask = (result_df[era_col] == era)\n",
        "                    if era_mask.sum() < 10:\n",
        "                        continue\n",
        "\n",
        "                    era_indices = result_df.loc[era_mask].index\n",
        "                    era_features = feature_df.loc[era_indices, neutral_cols]\n",
        "                    era_predictions = result_df.loc[era_mask, 'prediction'].values.reshape(-1, 1)\n",
        "\n",
        "                    try:\n",
        "                        # Ridge回帰で中立化（より安定）\n",
        "                        model = Ridge(alpha=0.01, fit_intercept=False).fit(era_features, era_predictions)\n",
        "                        neutralized = era_predictions - proportion * model.predict(era_features).reshape(-1, 1)\n",
        "                        result_df.loc[era_indices, 'prediction'] = neutralized.flatten()\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "    # 3. 簡略化したMMC最適化\n",
        "    if example_preds is not None:\n",
        "        try:\n",
        "            if 'prediction' in example_preds.columns:\n",
        "                merged = pd.merge(\n",
        "                    result_df[['id', 'prediction']],\n",
        "                    example_preds[['id', 'prediction']],\n",
        "                    on='id',\n",
        "                    suffixes=('', '_example')\n",
        "                )\n",
        "\n",
        "                # 相関が高い場合のみ差別化を適用（計算を節約）\n",
        "                corr = np.corrcoef(merged['prediction'], merged['prediction_example'])[0, 1]\n",
        "                print(f\"  メタモデルとの相関: {corr:.4f}\")\n",
        "\n",
        "                if corr > 0.75:  # 高い相関の場合のみ\n",
        "                    # 簡易的な差別化（より効率的）\n",
        "                    diff_strength = 0.2  # 固定値を使用して計算を簡略化\n",
        "\n",
        "                    # 直接計算（メモリマッピングを回避）\n",
        "                    merged['prediction_mmc'] = merged['prediction'] + diff_strength * (\n",
        "                        merged['prediction'] - merged['prediction_example'])\n",
        "\n",
        "                    # マッピングを更新\n",
        "                    prediction_map = dict(zip(merged['id'], merged['prediction_mmc']))\n",
        "                    result_df['prediction'] = result_df['id'].map(prediction_map).fillna(result_df['prediction'])\n",
        "        except Exception as e:\n",
        "            print(f\"MMC最適化エラー: {e}\")\n",
        "\n",
        "    # 4. 最終クリッピング\n",
        "    result_df['prediction'] = result_df['prediction'].clip(-3, 3)\n",
        "\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "OhiAE005KXSk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerai_lightweight_pipeline():\n",
        "    \"\"\"Colabの無料枠内で実行可能な軽量版ワークフロー\"\"\"\n",
        "    # データ読み込み（既存のコードを使用）\n",
        "    feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
        "    feature_cols = feature_metadata[\"feature_sets\"][\"medium\"]\n",
        "    target_cols = feature_metadata[\"targets\"]\n",
        "\n",
        "    # トレーニングデータの読み込みとスマートサンプリング\n",
        "    train = pd.read_parquet(\n",
        "        f\"{DATA_VERSION}/train.parquet\",\n",
        "        columns=[\"era\"] + feature_cols + target_cols\n",
        "    )\n",
        "    train = smart_sampling(train, era_col=\"era\", target_col=MAIN_TARGET, sample_ratio=0.25)\n",
        "\n",
        "    # 超効率的な特徴エンジニアリング\n",
        "    print(\"超効率的な特徴エンジニアリングを実行中...\")\n",
        "    train_engineered = ultra_efficient_feature_engineering(\n",
        "        train, feature_cols, era_col=\"era\", features_info=feature_metadata\n",
        "    )\n",
        "\n",
        "    # 高速特徴選択\n",
        "    print(\"高速特徴選択を実行中...\")\n",
        "    selected_features = ultra_fast_feature_selection(\n",
        "        train_engineered, MAIN_TARGET, feature_cols, era_col=\"era\"\n",
        "    )\n",
        "\n",
        "    # 軽量モデルトレーニング\n",
        "    print(\"軽量モデルトレーニングを実行中...\")\n",
        "    models = train_minimal_models(\n",
        "        train_engineered, selected_features, MAIN_TARGET, era_col=\"era\"\n",
        "    )\n",
        "\n",
        "    # トーナメントデータ処理\n",
        "    print(\"トーナメントデータを処理中...\")\n",
        "    tournament = pd.read_parquet(\n",
        "        f\"{DATA_VERSION}/live.parquet\",\n",
        "        columns=[\"era\", \"id\"] + feature_cols\n",
        "    )\n",
        "\n",
        "    # トーナメントデータに同じ特徴エンジニアリングを適用\n",
        "    tournament_engineered = ultra_efficient_feature_engineering(\n",
        "        tournament, feature_cols, era_col=\"era\", features_info=feature_metadata\n",
        "    )\n",
        "\n",
        "    # 予測生成\n",
        "    print(\"予測を生成中...\")\n",
        "    # 各モデルの予測を計算\n",
        "    model_preds = {}\n",
        "    for name, model in models.items():\n",
        "        if name != 'weights':\n",
        "            model_preds[name] = model.predict(tournament_engineered[selected_features])\n",
        "\n",
        "    # 重み付けアンサンブル\n",
        "    weights = models.get('weights', {})\n",
        "    ensemble_pred = np.zeros(len(tournament_engineered))\n",
        "    for name, preds in model_preds.items():\n",
        "        if name in weights:\n",
        "            ensemble_pred += weights[name] * preds\n",
        "        else:\n",
        "            ensemble_pred += preds / len(model_preds)\n",
        "\n",
        "    # 予測フォーマット\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'id': tournament_engineered[\"id\"],\n",
        "        'era': tournament_engineered[\"era\"],\n",
        "        'prediction': ensemble_pred\n",
        "    })\n",
        "\n",
        "    # 最適化\n",
        "    print(\"予測を最適化中...\")\n",
        "    # 例示予測の読み込み\n",
        "    example_preds = pd.read_parquet(f\"{DATA_VERSION}/live_example_preds.parquet\")\n",
        "\n",
        "    # 特徴グループ（メタデータから）\n",
        "    feature_groups = feature_metadata.get(\"feature_sets\", {})\n",
        "\n",
        "    # 効率的な予測最適化\n",
        "    optimized_preds = streamlined_prediction_optimization(\n",
        "        predictions_df,\n",
        "        tournament_engineered,\n",
        "        era_col=\"era\",\n",
        "        example_preds=example_preds,\n",
        "        feature_groups=feature_groups\n",
        "    )\n",
        "\n",
        "    # 結果を保存\n",
        "    final_predictions = optimized_preds[['id', 'prediction']].copy()\n",
        "    submission_path = f\"numerai_lightweight_submission.csv\"\n",
        "    final_predictions.to_csv(submission_path, index=False)\n",
        "\n",
        "    print(f\"軽量パイプライン完了! 予測保存先: {submission_path}\")\n",
        "\n",
        "    return final_predictions, models"
      ],
      "metadata": {
        "id": "4J3DH6X3KZU9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nan_safe_feature_selection(df, target_col, feature_cols, era_col=\"era\"):\n",
        "    \"\"\"NaN値に対応した特徴選択関数\"\"\"\n",
        "    from sklearn.linear_model import LassoCV\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    import numpy as np\n",
        "\n",
        "    print(\"NaN値に対応した特徴選択を実行中...\")\n",
        "\n",
        "    # 拡張特徴を検出\n",
        "    all_features = [col for col in df.columns\n",
        "                   if col.startswith('feature') or\n",
        "                   '_rank' in col or '_norm' in col or\n",
        "                   '_mul_' in col or 'pca_' in col or\n",
        "                   col.startswith('mean_')]\n",
        "\n",
        "    # 実際に存在する特徴のみ使用\n",
        "    valid_features = [f for f in all_features if f in df.columns]\n",
        "    print(f\"有効な特徴量: {len(valid_features)}\")\n",
        "\n",
        "    if len(valid_features) == 0:\n",
        "        print(\"警告: 有効な特徴量が見つかりません。基本特徴量を返します。\")\n",
        "        return feature_cols[:min(200, len(feature_cols))]\n",
        "\n",
        "    # サンプリングでメモリ効率化\n",
        "    sample_size = min(50000, len(df))\n",
        "    sample_idx = np.random.choice(len(df), sample_size, replace=False)\n",
        "    sample_df = df.iloc[sample_idx]\n",
        "\n",
        "    # 特徴行列とターゲット\n",
        "    X = sample_df[valid_features]\n",
        "    y = sample_df[target_col]\n",
        "\n",
        "    # NaN値のチェック\n",
        "    nan_count = X.isna().sum().sum()\n",
        "    print(f\"特徴行列中のNaN値の総数: {nan_count}\")\n",
        "\n",
        "    # NaN値を平均値で補完\n",
        "    print(\"欠損値を平均値で補完しています...\")\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "    # LassoCVによる特徴選択\n",
        "    print(\"LassoCVで特徴選択を実行中...\")\n",
        "    selector = LassoCV(cv=3, max_iter=100, tol=0.01, random_state=42)\n",
        "\n",
        "    try:\n",
        "        selector.fit(X_imputed, y)\n",
        "\n",
        "        # 特徴重要度\n",
        "        importance = np.abs(selector.coef_)\n",
        "        feature_importance = pd.Series(importance, index=valid_features)\n",
        "        feature_importance = feature_importance.sort_values(ascending=False)\n",
        "\n",
        "        # 重要な特徴のみを選択\n",
        "        selected_features = feature_importance.index[:min(200, len(feature_importance))].tolist()\n",
        "        print(f\"選択された特徴数: {len(selected_features)}\")\n",
        "\n",
        "        return selected_features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"モデル訓練中にエラー発生: {e}\")\n",
        "        print(\"代替手法として相関ベースの特徴選択を使用します...\")\n",
        "        return correlation_based_selection(sample_df, valid_features, target_col, max_features=200)\n",
        "\n",
        "def correlation_based_selection(df, features, target_col, max_features=200):\n",
        "    \"\"\"相関係数に基づく特徴選択（LassoCVのフォールバック）\"\"\"\n",
        "    correlations = {}\n",
        "\n",
        "    for feature in features:\n",
        "        # NaN値を除外して相関を計算\n",
        "        mask = ~(df[feature].isna() | df[target_col].isna())\n",
        "        if mask.sum() > 10:  # 十分なデータがある場合\n",
        "            try:\n",
        "                corr = np.abs(np.corrcoef(df.loc[mask, feature], df.loc[mask, target_col])[0, 1])\n",
        "                if not np.isnan(corr):\n",
        "                    correlations[feature] = corr\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # 相関でソート\n",
        "    sorted_features = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
        "    selected = [f for f, _ in sorted_features[:max_features]]\n",
        "\n",
        "    print(f\"相関ベースの選択で{len(selected)}個の特徴を選択\")\n",
        "    return selected"
      ],
      "metadata": {
        "id": "wFT-ZfDpS-Uq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def robust_minimal_models(train_df, features, target_col, era_col=\"era\"):\n",
        "    \"\"\"NaN値に対応した堅牢な軽量モデルトレーニング\"\"\"\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    from lightgbm import LGBMRegressor\n",
        "\n",
        "    print(\"NaN値に対応したモデルトレーニングを実行中...\")\n",
        "\n",
        "    # サンプリングでメモリ効率化\n",
        "    sample_size = min(50000, len(train_df))\n",
        "    sample_idx = np.random.choice(len(train_df), sample_size, replace=False)\n",
        "    sample_df = train_df.iloc[sample_idx]\n",
        "\n",
        "    # 特徴行列とターゲット\n",
        "    X = sample_df[features]\n",
        "    y = sample_df[target_col]\n",
        "\n",
        "    # NaN値のチェック\n",
        "    nan_count = X.isna().sum().sum()\n",
        "    print(f\"トレーニングデータのNaN値の総数: {nan_count}\")\n",
        "\n",
        "    # NaN値を平均値で補完\n",
        "    print(\"欠損値を平均値で補完しています...\")\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "    # 保存用にimputer保持\n",
        "\n",
        "    # 1. Ridge回帰 (安定性重視)\n",
        "    print(\"Ridge回帰モデルをトレーニング中...\")\n",
        "    ridge_model = Ridge(alpha=1.0, random_state=42)\n",
        "    ridge_model.fit(X_imputed, y)\n",
        "\n",
        "    # 2. 軽量版LightGBM\n",
        "    print(\"軽量版LightGBMモデルをトレーニング中...\")\n",
        "    lgbm_model = LGBMRegressor(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.02,\n",
        "        num_leaves=15,\n",
        "        max_depth=4,\n",
        "        colsample_bytree=0.7,\n",
        "        subsample=0.7,\n",
        "        verbose=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        lgbm_model.fit(X_imputed, y)\n",
        "        lgbm_trained = True\n",
        "    except Exception as e:\n",
        "        print(f\"LightGBMトレーニングエラー: {e}\")\n",
        "        print(\"LightGBMをスキップし、Ridgeのみを使用します\")\n",
        "        lgbm_trained = False\n",
        "\n",
        "    # モデルとimputer\n",
        "    models = {\n",
        "        'ridge': ridge_model,\n",
        "        'imputer': imputer\n",
        "    }\n",
        "\n",
        "    if lgbm_trained:\n",
        "        models['lgbm'] = lgbm_model\n",
        "        models['weights'] = {'ridge': 0.3, 'lgbm': 0.7}  # 経験的に良い比率\n",
        "    else:\n",
        "        models['weights'] = {'ridge': 1.0}  # Ridgeのみ\n",
        "\n",
        "    return models"
      ],
      "metadata": {
        "id": "zf0LAP4qTAmi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nan_safe_prediction(models, tournament_df, features, era_col=\"era\"):\n",
        "    \"\"\"NaN値に対応した予測生成\"\"\"\n",
        "    print(\"NaN値に対応した予測を生成中...\")\n",
        "\n",
        "    # 特徴行列抽出\n",
        "    X_test = tournament_df[features]\n",
        "\n",
        "    # NaN値のチェック\n",
        "    nan_count = X_test.isna().sum().sum()\n",
        "    print(f\"テストデータのNaN値の総数: {nan_count}\")\n",
        "\n",
        "    # 保存したimputerで補完\n",
        "    imputer = models['imputer']\n",
        "    X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "    # 各モデルの予測\n",
        "    model_preds = {}\n",
        "    weights = models['weights']\n",
        "\n",
        "    for name, model in models.items():\n",
        "        if name not in ['imputer', 'weights']:\n",
        "            model_preds[name] = model.predict(X_test_imputed)\n",
        "\n",
        "    # 重み付きアンサンブル\n",
        "    ensemble_pred = np.zeros(len(tournament_df))\n",
        "\n",
        "    for name, preds in model_preds.items():\n",
        "        if name in weights:\n",
        "            ensemble_pred += weights[name] * preds\n",
        "\n",
        "    # 予測フォーマット\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'id': tournament_df[\"id\"],\n",
        "        'era': tournament_df[era_col],\n",
        "        'prediction': ensemble_pred\n",
        "    })\n",
        "\n",
        "    return predictions_df\n",
        "\n",
        "def simplified_optimization(predictions_df, feature_df=None, era_col=\"era\",\n",
        "                           example_preds=None, feature_groups=None):\n",
        "    \"\"\"シンプル化した予測最適化\"\"\"\n",
        "    print(\"軽量化した予測最適化を実行中...\")\n",
        "    result_df = predictions_df.copy()\n",
        "\n",
        "    # 1. エラ内正規化（必須）\n",
        "    for era in result_df[era_col].unique():\n",
        "        era_mask = result_df[era_col] == era\n",
        "        if era_mask.sum() >= 2:\n",
        "            era_preds = result_df.loc[era_mask, 'prediction']\n",
        "            mean = np.mean(era_preds)\n",
        "            std = np.std(era_preds)\n",
        "            if std > 0:\n",
        "                result_df.loc[era_mask, 'prediction'] = (era_preds - mean) / std\n",
        "\n",
        "    # 2. MMC最適化（メタモデルとの差別化）- 最も効果的な部分\n",
        "    if example_preds is not None:\n",
        "        try:\n",
        "            merged = pd.merge(\n",
        "                result_df[['id', 'prediction']],\n",
        "                example_preds[['id', 'prediction']],\n",
        "                on='id',\n",
        "                suffixes=('', '_example')\n",
        "            )\n",
        "\n",
        "            # 相関計算\n",
        "            corr = np.corrcoef(merged['prediction'], merged['prediction_example'])[0, 1]\n",
        "            print(f\"メタモデルとの相関: {corr:.4f}\")\n",
        "\n",
        "            if corr > 0.7:  # 高相関の場合のみ差別化\n",
        "                # 簡易的な差別化\n",
        "                diff_strength = 0.2\n",
        "                merged['prediction_mmc'] = merged['prediction'] + diff_strength * (\n",
        "                    merged['prediction'] - merged['prediction_example'])\n",
        "\n",
        "                # IDによるマッピング更新\n",
        "                prediction_map = dict(zip(merged['id'], merged['prediction_mmc']))\n",
        "                result_df['prediction'] = result_df['id'].map(prediction_map).fillna(result_df['prediction'])\n",
        "        except Exception as e:\n",
        "            print(f\"MMC最適化エラー: {e}\")\n",
        "\n",
        "    # 3. 最終クリッピング\n",
        "    result_df['prediction'] = result_df['prediction'].clip(-3, 3)\n",
        "\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "H6k7Wq7XTDdc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerai_minimal_pipeline():\n",
        "    \"\"\"超軽量Numeraiパイプライン - NaN対応版とパス修正版\"\"\"\n",
        "    print(\"超軽量Numeraiパイプライン（NaN対応版）を開始...\")\n",
        "\n",
        "    # 1. まずファイルの存在確認\n",
        "    print(\"必要なファイルの存在を確認中...\")\n",
        "    required_files = [\n",
        "        \"train.parquet\",\n",
        "        \"features.json\",\n",
        "        \"live.parquet\",\n",
        "        \"live_example_preds.parquet\"\n",
        "    ]\n",
        "\n",
        "    # ファイルが存在するか確認し、存在しない場合はダウンロード\n",
        "    for file in required_files:\n",
        "        # まず直接ファイル名で確認\n",
        "        if os.path.exists(file):\n",
        "            print(f\"  ✓ {file} が存在します\")\n",
        "        # DATA_VERSIONディレクトリ内を確認\n",
        "        elif os.path.exists(f\"{DATA_VERSION}/{file}\"):\n",
        "            print(f\"  ✓ {DATA_VERSION}/{file} が存在します\")\n",
        "        else:\n",
        "            print(f\"  ✗ {file} が見つかりません。ダウンロードを試みます...\")\n",
        "            try:\n",
        "                napi = NumerAPI()\n",
        "                if DATA_VERSION == \"v5.0\":\n",
        "                    # v5.0の場合はパスを入れる\n",
        "                    napi.download_dataset(f\"{DATA_VERSION}/{file}\")\n",
        "                else:\n",
        "                    # その他の場合は直接ファイル名\n",
        "                    napi.download_dataset(file)\n",
        "                print(f\"    ダウンロード完了: {file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"    ダウンロードエラー: {e}\")\n",
        "                return None, None\n",
        "\n",
        "    # 2. 適切なパスでファイルを読み込む\n",
        "    print(\"データ読み込みとサンプリングを開始...\")\n",
        "\n",
        "    # ファイルパスを適切に判断する関数\n",
        "    def get_file_path(filename):\n",
        "        if os.path.exists(filename):\n",
        "            return filename\n",
        "        elif os.path.exists(f\"{DATA_VERSION}/{filename}\"):\n",
        "            return f\"{DATA_VERSION}/{filename}\"\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"ファイルが見つかりません: {filename}\")\n",
        "\n",
        "    # features.jsonの読み込み\n",
        "    feature_metadata = json.load(open(get_file_path(\"features.json\")))\n",
        "    feature_cols = feature_metadata[\"feature_sets\"][\"medium\"]\n",
        "    target_cols = feature_metadata[\"targets\"]\n",
        "\n",
        "    # トレーニングデータの読み込み\n",
        "    train = pd.read_parquet(\n",
        "        get_file_path(\"train.parquet\"),\n",
        "        columns=[\"era\"] + feature_cols + [MAIN_TARGET]\n",
        "    )\n",
        "\n",
        "    # スマートサンプリング\n",
        "    train_sampled = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]\n",
        "    print(f\"サンプリング後のデータサイズ: {len(train_sampled)}\")\n",
        "\n",
        "    # 3. 最小限の特徴エンジニアリング (以下は同じ)\n",
        "    print(\"最小限の特徴エンジニアリングを実行中...\")\n",
        "    train_engineered = simple_feature_engineering(train_sampled, era_col=\"era\")\n",
        "\n",
        "    # 4. NaN対応特徴選択\n",
        "    print(\"NaN対応特徴選択を実行中...\")\n",
        "    selected_features = nan_safe_feature_selection(\n",
        "        train_engineered, MAIN_TARGET, feature_cols, era_col=\"era\"\n",
        "    )\n",
        "\n",
        "    # 5. 堅牢なモデルトレーニング\n",
        "    print(\"堅牢なモデルトレーニングを実行中...\")\n",
        "    models = robust_minimal_models(\n",
        "        train_engineered, selected_features, MAIN_TARGET, era_col=\"era\"\n",
        "    )\n",
        "\n",
        "    # 6. トーナメントデータ処理 - パスを修正\n",
        "    print(\"トーナメントデータを処理中...\")\n",
        "    tournament = pd.read_parquet(\n",
        "        get_file_path(\"live.parquet\"),\n",
        "        columns=[\"era\", \"id\"] + feature_cols\n",
        "    )\n",
        "\n",
        "    # 同じ特徴エンジニアリングを適用\n",
        "    tournament_engineered = simple_feature_engineering(\n",
        "        tournament, era_col=\"era\"\n",
        "    )\n",
        "\n",
        "    # 7. 予測生成\n",
        "    print(\"予測を生成中...\")\n",
        "    predictions_df = nan_safe_prediction(\n",
        "        models, tournament_engineered, selected_features, era_col=\"era\"\n",
        "    )\n",
        "\n",
        "    # 8. 予測最適化 - パスを修正\n",
        "    print(\"予測を最適化中...\")\n",
        "    example_preds = pd.read_parquet(get_file_path(\"live_example_preds.parquet\"))\n",
        "\n",
        "    optimized_preds = simplified_optimization(\n",
        "        predictions_df,\n",
        "        era_col=\"era\",\n",
        "        example_preds=example_preds\n",
        "    )\n",
        "\n",
        "    # 9. 結果保存\n",
        "    final_predictions = optimized_preds[['id', 'prediction']].copy()\n",
        "    submission_path = f\"numerai_minimal_submission.csv\"\n",
        "    final_predictions.to_csv(submission_path, index=False)\n",
        "\n",
        "    print(f\"軽量パイプライン完了! 予測保存先: {submission_path}\")\n",
        "\n",
        "    return final_predictions, models"
      ],
      "metadata": {
        "id": "7ZIMSDucK8un"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_feature_engineering(df, era_col=\"era\"):\n",
        "    \"\"\"超簡易版の特徴エンジニアリング - NaN値に強い\"\"\"\n",
        "    print(\"簡易特徴エンジニアリングを開始...\")\n",
        "    result_df = df.copy()\n",
        "\n",
        "    # 特徴量を検出\n",
        "    feature_cols = [col for col in df.columns if col.startswith('feature')]\n",
        "    print(f\"検出された特徴量: {len(feature_cols)}\")\n",
        "\n",
        "    if len(feature_cols) == 0:\n",
        "        print(\"警告: 特徴量が見つかりません\")\n",
        "        return result_df\n",
        "\n",
        "    # 最小限の変換のみ実行\n",
        "    # 1. 上位20特徴に対してランク変換（NaN値に強い）\n",
        "    top_features = feature_cols[:min(20, len(feature_cols))]\n",
        "\n",
        "    for col in top_features:\n",
        "        # NaN値を含む可能性があるのでtry-exceptで囲む\n",
        "        try:\n",
        "            result_df[f'{col}_rank'] = result_df.groupby(era_col)[col].rank(pct=True)\n",
        "        except:\n",
        "            print(f\"Warning: {col}のランク変換に失敗\")\n",
        "\n",
        "    # 2. 最小限のクロス特徴（上位5特徴間の積）\n",
        "    for i, feat1 in enumerate(top_features[:5]):\n",
        "        for feat2 in top_features[i+1:min(i+3, len(top_features))]:\n",
        "            try:\n",
        "                result_df[f'{feat1}_mul_{feat2}'] = result_df[feat1] * result_df[feat2]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    print(\"簡易特徴エンジニアリング完了\")\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "NtOVr2tyTKON"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerai_minimal_pipeline():\n",
        "    \"\"\"超軽量Numeraiパイプライン - NaN対応版とパス修正版\"\"\"\n",
        "    print(\"超軽量Numeraiパイプライン（NaN対応版）を開始...\")\n",
        "\n",
        "    # 1. まずファイルの存在確認\n",
        "    print(\"必要なファイルの存在を確認中...\")\n",
        "    required_files = [\n",
        "        \"train.parquet\",\n",
        "        \"features.json\",\n",
        "        \"live.parquet\",\n",
        "        \"live_example_preds.parquet\"\n",
        "    ]\n",
        "\n",
        "    # ファイルが存在するか確認し、存在しない場合はダウンロード\n",
        "    for file in required_files:\n",
        "        # まず直接ファイル名で確認\n",
        "        if os.path.exists(file):\n",
        "            print(f\"  ✓ {file} が存在します\")\n",
        "        # DATA_VERSIONディレクトリ内を確認\n",
        "        elif os.path.exists(f\"{DATA_VERSION}/{file}\"):\n",
        "            print(f\"  ✓ {DATA_VERSION}/{file} が存在します\")\n",
        "        else:\n",
        "            print(f\"  ✗ {file} が見つかりません。ダウンロードを試みます...\")\n",
        "            try:\n",
        "                napi = NumerAPI()\n",
        "                if DATA_VERSION == \"v5.0\":\n",
        "                    # v5.0の場合はパスを入れる\n",
        "                    napi.download_dataset(f\"{DATA_VERSION}/{file}\")\n",
        "                else:\n",
        "                    # その他の場合は直接ファイル名\n",
        "                    napi.download_dataset(file)\n",
        "                print(f\"    ダウンロード完了: {file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"    ダウンロードエラー: {e}\")\n",
        "                return None, None\n",
        "\n",
        "    # 2. 適切なパスでファイルを読み込む\n",
        "    print(\"データ読み込みとサンプリングを開始...\")\n",
        "\n",
        "    # ファイルパスを適切に判断する関数\n",
        "    def get_file_path(filename):\n",
        "        if os.path.exists(filename):\n",
        "            return filename\n",
        "        elif os.path.exists(f\"{DATA_VERSION}/{filename}\"):\n",
        "            return f\"{DATA_VERSION}/{filename}\"\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"ファイルが見つかりません: {filename}\")\n",
        "\n",
        "    # features.jsonの読み込み\n",
        "    feature_metadata = json.load(open(get_file_path(\"features.json\")))\n",
        "    feature_cols = feature_metadata[\"feature_sets\"][\"medium\"]\n",
        "    target_cols = feature_metadata[\"targets\"]\n",
        "\n",
        "    # トレーニングデータの読み込み\n",
        "    train = pd.read_parquet(\n",
        "        get_file_path(\"train.parquet\"),\n",
        "        columns=[\"era\"] + feature_cols + [MAIN_TARGET]\n",
        "    )\n",
        "\n",
        "    # スマートサンプリング\n",
        "    train_sampled = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]\n",
        "    print(f\"サンプリング後のデータサイズ: {len(train_sampled)}\")\n",
        "\n",
        "    # 3. 最小限の特徴エンジニアリング (以下は同じ)\n",
        "    print(\"最小限の特徴エンジニアリングを実行中...\")\n",
        "    train_engineered = simple_feature_engineering(train_sampled, era_col=\"era\")\n",
        "\n",
        "    # 4. NaN対応特徴選択\n",
        "    print(\"NaN対応特徴選択を実行中...\")\n",
        "    selected_features = nan_safe_feature_selection(\n",
        "        train_engineered, MAIN_TARGET, feature_cols, era_col=\"era\"\n",
        "    )\n",
        "\n",
        "    # 5. 堅牢なモデルトレーニング\n",
        "    print(\"堅牢なモデルトレーニングを実行中...\")\n",
        "    models = robust_minimal_models(\n",
        "        train_engineered, selected_features, MAIN_TARGET, era_col=\"era\"\n",
        "    )\n",
        "\n",
        "    # 6. トーナメントデータ処理 - パスを修正\n",
        "    print(\"トーナメントデータを処理中...\")\n",
        "    tournament = pd.read_parquet(\n",
        "        get_file_path(\"live.parquet\"),\n",
        "        columns=[\"era\", \"id\"] + feature_cols\n",
        "    )\n",
        "\n",
        "    # 同じ特徴エンジニアリングを適用\n",
        "    tournament_engineered = simple_feature_engineering(\n",
        "        tournament, era_col=\"era\"\n",
        "    )\n",
        "\n",
        "    # 7. 予測生成\n",
        "    print(\"予測を生成中...\")\n",
        "    predictions_df = nan_safe_prediction(\n",
        "        models, tournament_engineered, selected_features, era_col=\"era\"\n",
        "    )\n",
        "\n",
        "    # 8. 予測最適化 - パスを修正\n",
        "    print(\"予測を最適化中...\")\n",
        "    example_preds = pd.read_parquet(get_file_path(\"live_example_preds.parquet\"))\n",
        "\n",
        "    optimized_preds = simplified_optimization(\n",
        "        predictions_df,\n",
        "        era_col=\"era\",\n",
        "        example_preds=example_preds\n",
        "    )\n",
        "\n",
        "    # 9. 結果保存\n",
        "    final_predictions = optimized_preds[['id', 'prediction']].copy()\n",
        "    submission_path = f\"numerai_minimal_submission.csv\"\n",
        "    final_predictions.to_csv(submission_path, index=False)\n",
        "\n",
        "    print(f\"軽量パイプライン完了! 予測保存先: {submission_path}\")\n",
        "\n",
        "    return final_predictions, models"
      ],
      "metadata": {
        "id": "VKiISspeTMlJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# osモジュールをインポート\n",
        "import os\n",
        "\n",
        "# 全ての関数を定義した後、最後にこれを実行\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"パス修正版の超軽量Numeraiパイプラインを実行します...\")\n",
        "    final_predictions, models = numerai_minimal_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9NadS-eXWqZ",
        "outputId": "1f47da0b-f8ea-42f9-841f-a9015c3a3208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "パス修正版の超軽量Numeraiパイプラインを実行します...\n",
            "超軽量Numeraiパイプライン（NaN対応版）を開始...\n",
            "必要なファイルの存在を確認中...\n",
            "  ✓ v5.0/train.parquet が存在します\n",
            "  ✓ v5.0/features.json が存在します\n",
            "  ✗ live.parquet が見つかりません。ダウンロードを試みます...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "v5.0/live.parquet: 8.03MB [00:00, 19.2MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ダウンロード完了: live.parquet\n",
            "  ✓ v5.0/live_example_preds.parquet が存在します\n",
            "データ読み込みとサンプリングを開始...\n",
            "サンプリング後のデータサイズ: 688184\n",
            "最小限の特徴エンジニアリングを実行中...\n",
            "簡易特徴エンジニアリングを開始...\n",
            "検出された特徴量: 705\n",
            "簡易特徴エンジニアリング完了\n",
            "NaN対応特徴選択を実行中...\n",
            "NaN値に対応した特徴選択を実行中...\n",
            "有効な特徴量: 735\n",
            "特徴行列中のNaN値の総数: 0\n",
            "欠損値を平均値で補完しています...\n",
            "LassoCVで特徴選択を実行中...\n"
          ]
        }
      ]
    }
  ]
}